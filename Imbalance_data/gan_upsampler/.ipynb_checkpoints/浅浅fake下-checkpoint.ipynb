{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from Automan.plugins.risk_trend_plot import plt_multi_rsk_trend\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_update = pd.read_csv('dq_label_update.csv')\n",
    "# inputx_online_update = pd.read_csv('dq_inputx_online_update.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputx_online_update.drop(columns=['create_datetime'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zixin_all = pd.read_csv('dq_zx_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tmp1_update = pd.merge(label_update,inputx_online_update,how='left',on='cust_no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tmp3_update = pd.merge(df_tmp1_update,zixin_all,how='left',on='cust_no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tmp4_update = df_tmp3_update[df_tmp3_update.fpd4!=-1].reset_index(drop=True)\n",
    "# df_tmp5_update = df_tmp4_update[df_tmp4_update.mob3_k11!=-1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oot_202208_update = df_tmp4_update[(df_tmp4_update.create_datetime>='2022-08-01')&(df_tmp4_update.create_datetime<='2022-08-31')].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tmp5_update = df_tmp5_update.append(oot_202208_update).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 包括所有资信\n",
    "# feat_li = list(set(list(df_tmp5_update)) - set(['cust_no','create_datetime','month','fpd4','mob3_k4','mob3_k11']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 区分数值型变量和非数值型\n",
    "\n",
    "# float_feats = list()\n",
    "\n",
    "# for f in feat_li:\n",
    "#     if df_tmp5_update[f].dtype == 'float64':\n",
    "#         float_feats.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for f in float_feats:\n",
    "#     df_tmp5_update[f].fillna(-99,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tmp6 = df_tmp5_update[float_feats + ['cust_no','create_datetime','fpd4','mob3_k4','mob3_k11']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp6['month'] = df_tmp6.create_datetime.apply(lambda x:x[:4]+x[5:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp6.to_csv('df_tmp6.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df = df_tmp6[df_tmp6.create_datetime<='2022-04-30'].reset_index(drop=True)\n",
    "oot_df = df_tmp6[df_tmp6.create_datetime>='2022-05-01'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_use = ['pic_zm_score',\n",
    " 'pic_zm_upload_source',\n",
    " 'bqs_qly_feature18',\n",
    " 'bqs_qly_feature35',\n",
    " 'bqs_qly_feature13',\n",
    " 'idcard_district',\n",
    " 'upa_max_score',\n",
    " 'upa_total_deal_amt_low_limit_12m',\n",
    " 'upa_recent_deal_date_12m',\n",
    " 'upa_earliest_deal_date_from_201101',\n",
    " 'upa_deal_amt_ecpt_wholesale_6m',\n",
    " 'upa_min_deal_amt_12m',\n",
    " 'upa_consume_amt_bt_9_to_18_3m',\n",
    " 'upa_deal_amt_ecpt_wholesale_12m',\n",
    " 'upa_max_consume_amt_6m',\n",
    " 'upa_consume_amt_bt_9_to_18_12m',\n",
    " 'upa_deal_amt_in_work_day_and_hours_3m',\n",
    " 'pre_loan_install',\n",
    " 'pre_loan_amount',\n",
    " 'cl_br_sd_scorecust_gl_m1_min_group_num',\n",
    " 'cl_br_sd_scorecust_gl_m1_max_group_num',\n",
    " 'mobile_6m_cnt',\n",
    " 'telephone_bj_1m_sum',\n",
    " 'ope_weehours_calls_rate',\n",
    " 'ope_hifreq_object_calls',\n",
    " 'ope_midnight_calls_rate',\n",
    " 'Y_rotation_angle',\n",
    " 'Z_rotation_angle',\n",
    " 'X_rotation_angle',\n",
    " 'ads_contact_in_call_cnt',\n",
    " 'ads_contact_in_call_pt',\n",
    " 'ads_mp_contact_in_call_pt',\n",
    " 'ads_mp_call_in_contact_pt',\n",
    " 'bda_best_cust_flag',\n",
    " 'bda_applist_cnt',\n",
    " 'upa_rencent_fund_shortage_date_12m',\n",
    " 'upa_rencent_faied_deal_date_12m',\n",
    " 'cl_br_sd_scorecust_gl_m12_min_group_num',\n",
    " 'cl_br_sd_scorecust_gl_m12_max_group_num',\n",
    " 'bwjk_gxwl_level',\n",
    " 'name_all_letter',\n",
    " 'name_len_bgthan_3',\n",
    " 'name_contain_bracket_ratio',\n",
    " 'name_contain_jie',\n",
    " 'mob_netflow',\n",
    " 'name_contain_laopo_ratio',\n",
    " 'relation_type_3_0_ratio',\n",
    " 'upa_failed_deal_cnt_6m',\n",
    " 'upa_other_service_cnt_12m',\n",
    " 'callb_fraud_level_jdn',\n",
    " 'callb_credit_level_jdn',\n",
    " 'cs_gy_hcf',\n",
    " 'mob_contain_4_ratio',\n",
    " 'name_contain_shifu',\n",
    " 'mob_dianxin_ratio',\n",
    " 'name_contain_normstr_ratio',\n",
    " 'n1txl_p08jiaoc047cf8',\n",
    " 'mob_netflow_ratio',\n",
    " 'mob_total_3_ratio',\n",
    " 'name_most_dup',\n",
    " 'mob_first7_most_dup_ratio',\n",
    " 'name_contain_yifu_ratio',\n",
    " 'n1txl_p08jiaoc027cf2',\n",
    " 'name_contain_ma_ratio',\n",
    " 'mob_total_8_ratio',\n",
    " 'name_contain_gongsi_ratio',\n",
    " 'name_contain_ba_ratio',\n",
    " 'mob_liantong',\n",
    " 'name_contain_yi_ratio',\n",
    " 'name_contain_laogong_ratio',\n",
    " 'name_contain_jiu_ratio',\n",
    " 'mob_contain_9_ratio',\n",
    " 'name_contain_jie_ratio',\n",
    " 'n1txl_p08jiaoc017cfy',\n",
    " 'name_contain_ge_ratio',\n",
    " 'mob_contain_7_ratio',\n",
    " 'mob_virt_ope_ratio',\n",
    " 'name_contain_shifu_ratio',\n",
    " 'mob_total_1_ratio',\n",
    " 'name_contain_ge',\n",
    " 'name_contain_zhuren_ratio',\n",
    " 'n1txl_p08jiaoc002cf2',\n",
    " 'name_begnwith_da_ratio',\n",
    " 'name_contain_laoban_ratio',\n",
    " 'mob_contain_2_ratio',\n",
    " 'mob_total_2_ratio',\n",
    " 'name_dup_2',\n",
    " 'name_contain_baba_ratio',\n",
    " 'relation_city_ratio',\n",
    " 'name_contain_laoshi_ratio',\n",
    " 'relation_mob_city_same_ratio',\n",
    " 'ads_CtactFamilyName_rank1_2_rate',\n",
    " 'ads_CtactFamilyName_rank1_rate',\n",
    " 'immediate_relation_cnt',\n",
    " 'ab_mobile_ratio',\n",
    " 'ab_local_ratio',\n",
    " 'ab_local_cnt',\n",
    " 'cs_br_dzf2',\n",
    " 'nopboc_model_msg',\n",
    " 'selffill_degree',\n",
    " 'cs_hnsk_bzf',\n",
    " 'new_reflecting_back_id_score',\n",
    " 'new_reflecting_front_id_score',\n",
    " 'birth_year',\n",
    " 'area_risk_level',\n",
    " 'cust_birth_date',\n",
    " 'selffill_marital_status',\n",
    " 'app_sum_cnt',\n",
    " 'cust_id_area',\n",
    " 'cust_work_city']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_x = train_val_df[feats_use + ['month']]\n",
    "train_val_y = train_val_df[['cust_no','mob3_k11','mob3_k4','fpd4']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "oot_x = oot_df[feats_use + ['month']]\n",
    "oot_y = oot_df[['cust_no','mob3_k11','fpd4']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,val_x,train_y,val_y = train_test_split(train_val_x,train_val_y,test_size=0.2,stratify=train_val_y['mob3_k11'],random_state=168)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.replace([-99,-999,-9999,-9999997],-99,inplace=True)\n",
    "val_x.replace([-99,-999,-9999,-9999997],-99,inplace=True)\n",
    "oot_x.replace([-99,-999,-9999,-9999997],-99,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.reset_index(drop=True)\n",
    "val_x = val_x.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_y.reset_index(drop=True)\n",
    "val_y = val_y.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_y = pd.concat([train_x,train_y],axis=1)\n",
    "val_x_y = pd.concat([val_x,val_y],axis=1)\n",
    "oot_x_y = pd.concat([oot_x,oot_y],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "\n",
    "class Data_Prep():\n",
    "\n",
    "    '''\n",
    "    the pre-processing of input data and post-processing of generated data\n",
    "    '''\n",
    "    def __init__(self,raw_df,categorial,log,mixed,integer,types,test_ratio):\n",
    "\n",
    "        self.categorical_cols = categorial \n",
    "        self.log_cols = log # list of skewed exponential numerical columns\n",
    "        self.mixed_cols = mixed # list of mixed type columns ---> BUT actually columns with nan\n",
    "        self.integer_cols = integer # list of numeric columns without floating number\n",
    "\n",
    "        self.col_types = dict()\n",
    "        self.col_types['categorical'] = list()\n",
    "        self.col_types['mixed'] = dict()\n",
    "\n",
    "        self.lower_bounds = dict()\n",
    "        self.label_encoder_list = list()\n",
    "\n",
    "\n",
    "        target_col = list(types.values())[0]\n",
    "        \n",
    "        y_real = raw_df[target_col]\n",
    "        X_real = raw_df.drop(columns=[target_col])\n",
    "\n",
    "        X_train_real,_,y_train_real,_ = model_selection.train_test_split(X_real,y_real,test_size=test_ratio,stratify=y_real,random_state=100)\n",
    "        X_train_real[target_col] = y_train_real\n",
    "\n",
    "\n",
    "        self.df = X_train_real\n",
    "        # self.df.replace(r'',np.nan,inplace=True)\n",
    "        # self.df.fillna('-99',inplace=True)\n",
    "\n",
    "        # all_cols = set(self.df.columns)\n",
    "        # irrelevant_missing_cols = set(self.categorical_cols)\n",
    "        # relevant_missing_cols = list(all_cols - irrelevant_missing_cols)\n",
    "\n",
    "        # for i in relevant_missing_cols:\n",
    "        #     if i in list(self.mixed_cols.keys()):\n",
    "        #         self.df[i] = self.df[i].apply(lambda x: -99 if x=='-99' else x)\n",
    "        \n",
    "        # log transformer for skewed exp numeric dist\n",
    "        if self.log_cols:\n",
    "            for log_col in self.log_cols:\n",
    "                eps = 1\n",
    "                lower = np.min(self.df.loc[self.df[log_col]!=-99][log_col].values)\n",
    "                self.lower_bounds[log_col] = lower\n",
    "                if lower > 0:\n",
    "                    self.df[log_col] = self.df[log_col].apply(lambda x:np.log(x) if x!= -99 else -99)\n",
    "                elif lower == 0:\n",
    "                    self.df[log_col] = self.df[log_col].apply(lambda x:np.log(x+eps) if x!=-99 else -99)\n",
    "                else:\n",
    "                    self.df[log_col] = self.df[log_col].apply(lambda x:np.log(x-lower+eps) if x!=-99 else -99)\n",
    "        \n",
    "\n",
    "\n",
    "        for idx,col in enumerate(self.df.columns):\n",
    "\n",
    "            if col in self.categorical_cols:\n",
    "                label_encoder = preprocessing.LabelEncoder()\n",
    "                self.df[col] = self.df[col].astype(str)\n",
    "                label_encoder.fit(self.df[col])\n",
    "\n",
    "                curr_label_encoder = dict()\n",
    "                curr_label_encoder['col'] = col\n",
    "                curr_label_encoder['label_encoder'] = label_encoder\n",
    "\n",
    "                transformed_col = label_encoder.transform(self.df[col])\n",
    "                self.df[col] = transformed_col\n",
    "\n",
    "                self.label_encoder_list.append(curr_label_encoder)\n",
    "                self.col_types['categorical'].append(idx)\n",
    "            \n",
    "            elif col in self.mixed_cols:\n",
    "                self.col_types['mixed'][idx] = self.mixed_cols[col]\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "    \n",
    "    def inverse_prep(self,data,eps=1):\n",
    "\n",
    "        df_sample = pd.DataFrame(data,columns=self.df.columns)\n",
    "\n",
    "        for i in range(len(self.label_encoder_list)):\n",
    "            le = self.label_encoder_list[i]['label_encoder']\n",
    "            df_sample[self.label_encoder_list[i]['col']] = df_sample[self.label_encoder_list[i]['col']].astype(int)\n",
    "            df_sample[self.label_encoder_list[i]['col']] = le.inverse_transform(df_sample[self.label_encoder_list[i]['col']])\n",
    "\n",
    "        \n",
    "        if self.log_cols:\n",
    "            for i in df_sample:\n",
    "                if i in self.log_cols:\n",
    "                    lower_bound = self.lower_bounds[i]\n",
    "                    if lower_bound > 0:\n",
    "                        df_sample[i] = df_sample[i].apply(lambda x:np.exp(x) if x!=-99 else -99)\n",
    "                    elif lower_bound == 0:\n",
    "                        df_sample[i] = df_sample[i].apply(lambda x: np.ceil(np.exp(x)-eps) if ((x!=-99) & ((np.exp(x)-eps) < 0)) else (np.exp(x)-eps if x!=-99 else -99))\n",
    "                    else: \n",
    "                        df_sample[i] = df_sample[i].apply(lambda x: np.exp(x)-eps+lower_bound if x!=-99 else -99)\n",
    "        \n",
    "        if self.integer_cols:\n",
    "            for col in self.integer_cols:\n",
    "                df_sample[col] = (np.round(df_sample[col].values))\n",
    "                df_sample[col] = df_sample[col].astype(int)\n",
    "        \n",
    "        return df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class D_Transformer():\n",
    "\n",
    "\n",
    "    def __init__(self, train_data, categorical_cols=[], mixed_dict={}, n_clusters=5, threshold=0.005):\n",
    "        \n",
    "        self.meta = None\n",
    "        self.train_data = train_data\n",
    "        self.categorical_columns= categorical_cols\n",
    "        self.mixed_columns= mixed_dict\n",
    "        self.n_clusters = n_clusters\n",
    "        self.threshold = threshold\n",
    "        self.ordering = []\n",
    "        self.output_info = []\n",
    "        self.output_dim = 0\n",
    "        self.components = []\n",
    "        self.is_cont = []\n",
    "        \n",
    "        self.meta = self.get_metadata()\n",
    "    \n",
    "    def get_metadata(self):\n",
    "        '''\n",
    "        获取table中每个特征列的基本属性信息\n",
    "        '''\n",
    "        \n",
    "        meta = []\n",
    "    \n",
    "        for index in range(self.train_data.shape[1]):\n",
    "            column = self.train_data.iloc[:,index]\n",
    "            # 类别型特征\n",
    "            if index in self.categorical_columns:\n",
    "                mapper = column.value_counts().index.tolist()\n",
    "                meta.append({\n",
    "                        \"name\": index,\n",
    "                        \"type\": \"categorical\",\n",
    "                        \"size\": len(mapper),\n",
    "                        \"i2s\": mapper\n",
    "                })\n",
    "            # 混合类型(包含缺失值)\n",
    "            elif index in self.mixed_columns.keys():\n",
    "                meta.append({\n",
    "                    \"name\": index,\n",
    "                    \"type\": \"mixed\",\n",
    "                    \"min\": column.min(),\n",
    "                    \"max\": column.max(),\n",
    "                    \"modal\": self.mixed_columns[index]\n",
    "                })\n",
    "            # 连续型特征\n",
    "            else:\n",
    "                meta.append({\n",
    "                    \"name\": index,\n",
    "                    \"type\": \"continuous\",\n",
    "                    \"min\": column.min(),\n",
    "                    \"max\": column.max(),\n",
    "                })            \n",
    "\n",
    "        return meta\n",
    "    \n",
    "    def fit(self):\n",
    "\n",
    "        data = self.train_data.values\n",
    "\n",
    "        # 保存每一个训练的bgm模型\n",
    "        models = []\n",
    "\n",
    "        for idx,info in enumerate(self.meta):\n",
    "            if info['type'] == 'continuous':\n",
    "                gm = BayesianGaussianMixture(\n",
    "                    self.n_clusters,\n",
    "                    weight_concentration_prior_type='dirichlet_process',\n",
    "                    weight_concentration_prior=0.001,\n",
    "                    max_iter=100,n_init=1,random_state=100)\n",
    "                \n",
    "                gm.fit(data[:,idx].reshape([-1,1]))\n",
    "                models.append(gm)\n",
    "\n",
    "                mode_weights_above_threshold = gm.weights_ > self.threshold\n",
    "                mode_freq = (pd.Series(gm.predict(data[:,idx].reshape([-1,1]))).value_counts().keys())\n",
    "\n",
    "                modes = []\n",
    "                for i in range(self.n_clusters):\n",
    "                    if (i in mode_freq) & mode_weights_above_threshold[i]:\n",
    "                        modes.append(True)\n",
    "                    else:\n",
    "                        modes.append(False)\n",
    "                self.components.append(modes)\n",
    "                self.output_info += [(1,'tanh'),(np.sum(modes),'softmax')]\n",
    "                self.output_dim += 1 + np.sum(modes)\n",
    "            \n",
    "            elif info['type'] == 'mixed':\n",
    "                gm = BayesianGaussianMixture(\n",
    "                    self.n_clusters,\n",
    "                    weight_concentration_prior_type='dirichlet_process',\n",
    "                    weight_concentration_prior=0.001, max_iter=100,\n",
    "                    n_init=1,random_state=100)\n",
    "\n",
    "                is_cont = []\n",
    "                for ele in data[:,idx]:\n",
    "                    if ele not in info['modal']:\n",
    "                        is_cont.append(True)\n",
    "                    else:\n",
    "                        is_cont.append(False)\n",
    "                self.is_cont.append(is_cont)\n",
    "\n",
    "\n",
    "                gm.fit(data[:,idx][is_cont].reshape([-1,1]))\n",
    "\n",
    "                models.append(gm)\n",
    "\n",
    "                mode_weights_above_threshold = gm.weights_ > self.threshold\n",
    "                mode_freq = (pd.Series(gm.predict(data[:,idx][is_cont].reshape([-1,1]))).value_counts().keys())\n",
    "                modes = []\n",
    "\n",
    "                for i in range(self.n_clusters):\n",
    "                    if (i in mode_freq) & mode_weights_above_threshold[i]:\n",
    "                        modes.append(True)\n",
    "                    else:\n",
    "                        modes.append(False)\n",
    "                \n",
    "                self.components.append(modes)\n",
    "\n",
    "                self.output_info += [(1,'tanh'),(np.sum(modes) + len(info['modal']),'softmax')]\n",
    "                self.output_dim += 1 + np.sum(modes) + len(info['modal'])\n",
    "            \n",
    "            else: # categorial\n",
    "                model.append(None)\n",
    "                self.components.append(None)\n",
    "                self.output_info += [(info['size'],'softmax')]\n",
    "                self.output_dim += info['size']\n",
    "        \n",
    "        self.models = models\n",
    "\n",
    "    def transform(self,data):\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        mixed_counter = 0\n",
    "\n",
    "        for idx,info in enumerate(self.meta):\n",
    "            whereiam = data[:,idx]\n",
    "\n",
    "            if info['type'] == 'continuous':\n",
    "                whereiam = whereiam.reshape([-1,1])\n",
    "\n",
    "                # 获取每个高斯的均值\n",
    "                means = self.models[idx].means_.reshape((1,self.n_clusters))\n",
    "                # 获取每个高斯的标准差\n",
    "                stds =  np.sqrt(self.models[idx].covariances_).reshape((1,self.n_clusters))\n",
    "\n",
    "                features = np.empty(shape=(len(whereiam),self.n_clusters))\n",
    "                # 标准化(broadcasting)\n",
    "                features = (whereiam - means) / (4 * stds)\n",
    "\n",
    "                # 候选modes\n",
    "                num_opts = sum(self.components[idx])\n",
    "                \n",
    "                mode_select = np.zeros(len(data),dtype='int')\n",
    "                # 计算每条样本属于每个候选高斯分布的概率\n",
    "                probs = self.models[idx].predict_proba(whereiam.reshape([-1,1]))\n",
    "                # 剔除低贡献分布\n",
    "                probs = probs[:,self.components[idx]]\n",
    "                \n",
    "                # 为每条样本选择最匹配的mode\n",
    "                for i in range(len(data)):\n",
    "                    prob = probs[i] + 1e-5\n",
    "                    prob = prob / sum(prob)\n",
    "\n",
    "                    mode_select[i] = np.random.choice(np.arange(num_opts),p=prob)\n",
    "                \n",
    "                # one-hot encoding\n",
    "                probs_onehot = np.zeros_like(probs)\n",
    "                probs_onehot[np.arange(len(probs)),mode_select] = 1\n",
    "\n",
    "                # 获取对应mode的标准化值\n",
    "                index = np.arange((len(features)))\n",
    "                features = features[:,self.components[idx]]\n",
    "                features = features[index,mode_select].reshape([-1,1])\n",
    "                features = np.clip(features,-0.99,0.99)\n",
    "\n",
    "                # re-order\n",
    "                ordered_probs_onehot = np.zeros_like(probs_onehot)\n",
    "                col_sums = probs_onehot.sum(axis=0)\n",
    "\n",
    "                n = probs_onehot.shape[1]\n",
    "                largest_indices = np.argsort(-1*col_sums)[:n]\n",
    "\n",
    "                for index_,val in enumerate(largest_indices):\n",
    "                    ordered_probs_onehot[:,index_] = probs_onehot[:,val]\n",
    "                \n",
    "                self.ordering.append(largest_indices)\n",
    "\n",
    "                # 存储编码\n",
    "                outputs += [features,ordered_probs_onehot]\n",
    "\n",
    "            elif info['type'] == 'mixed':\n",
    "                \n",
    "                whereiam = whereiam.reshape([-1,1])\n",
    "                is_cont = self.is_cont[mixed_counter] # 获取该mixed特征的连续位置\n",
    "\n",
    "                whereiam = whereiam[is_cont]\n",
    "\n",
    "                # 获取每个高斯的均值\n",
    "                means = self.models[idx].means_.reshape((1,self.n_clusters))\n",
    "                # 获取每个高斯的标准差\n",
    "                stds =  np.sqrt(self.models[idx].covariances_).reshape((1,self.n_clusters))\n",
    "\n",
    "                features = np.empty(shape=(len(whereiam),self.n_clusters))\n",
    "                # 标准化(broadcasting)\n",
    "                features = (whereiam - means) / (4 * stds)\n",
    "\n",
    "                # 候选modes\n",
    "                num_opts = sum(self.components[idx])\n",
    "                \n",
    "                mode_select = np.zeros(len(whereiam),dtype='int')\n",
    "                # 计算每条样本属于每个候选高斯分布的概率\n",
    "                probs = self.models[idx].predict_proba(whereiam.reshape([-1,1]))\n",
    "                # 剔除低贡献分布\n",
    "                probs = probs[:,self.components[idx]]\n",
    "                \n",
    "                # 为每条样本选择最匹配的mode\n",
    "                for i in range(len(whereiam)):\n",
    "                    prob = probs[i] + 1e-5\n",
    "                    prob = prob / sum(prob)\n",
    "\n",
    "                    mode_select[i] = np.random.choice(np.arange(num_opts),p=prob)\n",
    "                \n",
    "                # one-hot encoding\n",
    "                probs_onehot = np.zeros_like(probs)\n",
    "                probs_onehot[np.arange(len(probs)),mode_select] = 1\n",
    "\n",
    "                # 获取对应mode的标准化值\n",
    "                index = np.arange((len(features)))\n",
    "                features = features[:,self.components[idx]]\n",
    "                features = features[index,mode_select].reshape([-1,1])\n",
    "                features = np.clip(features,-0.99,0.99)\n",
    "\n",
    "                # 为缺失值添加一种额外的mode\n",
    "                extra_mode = np.zeros([len(whereiam),len(info['modal'])])\n",
    "                probs_onehot_tmp = np.concatenate([extra_mode,probs_onehot],axis=1)\n",
    "\n",
    "                # 最终输出\n",
    "                fin_out = np.zeros([len(data), 1 + len(info['modal']) + probs_onehot.shape[1]])\n",
    "\n",
    "                features_curser = 0\n",
    "                for index_,val in enumerate(data[:,idx]):\n",
    "                    if val in info['modal']: # -99\n",
    "                        cate_ = list(map(info['modal'].index, [val]))[0]\n",
    "                        fin_out[index_,0] = -99  # 直接将缺失值作为feature value\n",
    "                        fin_out[index_,(cate_+1)] = 1\n",
    "                    else:\n",
    "                        fin_out[index_,0] = features[features_curser]\n",
    "                        fin_out[index_,(1 + len(info['modal'])):] = probs_onehot_tmp[features_curser][len(info['modal']):]\n",
    "                        features_curser += 1\n",
    "                \n",
    "                just_onehot = fin_out[:,1:]\n",
    "                ordered_just_onehot = np.zeros_like(just_onehot)\n",
    "\n",
    "                n = just_onehot.shape[1]\n",
    "                col_sums =just_onehot.sum(axis=0)\n",
    "                largest_indices = np.argsort(-1*col_sums)[:n]\n",
    "\n",
    "                for index_,val in enumerate(largest_indices):\n",
    "                    ordered_just_onehot[:,index_] = just_onehot[:,val]\n",
    "                \n",
    "                fin_features = fin_out[:,0].reshape([-1,1])\n",
    "\n",
    "                self.ordering.append(largest_indices)\n",
    "                outputs += [fin_features,ordered_just_onehot]\n",
    "\n",
    "                mixed_counter += 1\n",
    "            \n",
    "            else:\n",
    "                # category(可直接将missing value作为一类)\n",
    "                self.ordering.append(None)\n",
    "                out_cols = np.zeros([len(data),info['size']])\n",
    "                index = list(map(info['i2s'].index, whereiam))\n",
    "                out_cols[np.arange(len(data)),index] = 1\n",
    "                outputs.append(out_cols)\n",
    "        \n",
    "        return np.concatenate(outputs,axis=1)\n",
    "\n",
    "    def inverse_transform(self,data):\n",
    "        '''\n",
    "        data: generated data by GAN\n",
    "        '''   \n",
    "        # 存储转换回来的表数据\n",
    "        data_trans_back = np.zeros([len(data),len(self.meta)])\n",
    "\n",
    "        step = 0 # 遍历所有原始特征\n",
    "\n",
    "        for idx,info in enumerate(self.meta):\n",
    "            if info['type'] == 'continuous':\n",
    "\n",
    "                # 获取feature值\n",
    "                feature_val = data[:,step]\n",
    "                feature_val = np.clip(feature_val,-1,1)\n",
    "                \n",
    "                # 获取one-hot\n",
    "                onehot_val = data[:,step+1:step+1+np.sum(self.components[idx])]\n",
    "                \n",
    "                # order back\n",
    "                order = self.ordering[idx]\n",
    "                onehot_val_re_order = np.zeros_like(onehot_val)    \n",
    "                for index,val in enumerate(order):\n",
    "                    onehot_val_re_order[:,val] = onehot_val[:,index]\n",
    "\n",
    "                onehot_val = onehot_val_re_order\n",
    "\n",
    "                onehot_val_t = np.ones((data.shape[0],self.n_clusters)) * -100\n",
    "                onehot_val_t[:,self.components[idx]] = onehot_val\n",
    "\n",
    "                onehot_val = onehot_val_t\n",
    "\n",
    "                means = self.models[idx].means_.reshape([-1])\n",
    "                stds = np.sqrt(self.models[idx].covariances_).reshape([-1])\n",
    "                p_argmax = np.argmax(onehot_val,axis=1)\n",
    "                \n",
    "                std = stds[p_argmax]\n",
    "                mean = means[p_argmax]\n",
    "\n",
    "                # inverse transformation\n",
    "                inverse_feature_val = feature_val * 4 * std + mean\n",
    "\n",
    "                data_trans_back[:,idx] = inverse_feature_val\n",
    "\n",
    "                step += 1 + np.sum(self.components[idx])\n",
    "            \n",
    "            elif info['type'] == 'mixed':\n",
    "\n",
    "                feature_val = data[:,step]\n",
    "                feature_val = np.clip(feature_val,-1,1)\n",
    "\n",
    "                # 获取one-hot编码\n",
    "                full_onehot_val = data[:,(step+1):(step+1)+len(info['modal'])+np.sum(self.components[idx])]\n",
    "\n",
    "                order = self.ordering[idx]\n",
    "                full_onehot_val_re_order = np.zeros_like(full_onehot_val)\n",
    "                for index,val in enumerate(order):\n",
    "                    full_onehot_val_re_order[:,val] = full_onehot_val[:,index]\n",
    "                \n",
    "                full_onehot_val = full_onehot_val_re_order\n",
    "\n",
    "                # modes of categorical value ---> -99 \n",
    "                mixed_onehot_val = full_onehot_val[:,:len(info['modal'])] # 位于最前\n",
    "                # modes of continuous value\n",
    "                continuous_onehot_val = full_onehot_val[:,-np.sum(self.components[idx]):]\n",
    "\n",
    "                onehot_val_t = np.ones((data.shape[0],self.n_clusters)) * -100\n",
    "                onehot_val_t[:,self.components[idx]] = continuous_onehot_val\n",
    "\n",
    "\n",
    "                final_onehot_val = np.concatenate([mixed_onehot_val,onehot_val_t],axis=1)\n",
    "                p_argmax = np.argmax(final_onehot_val,axis=1)\n",
    "\n",
    "                # 对于continuous变量计算\n",
    "                means = self.models[idx].means_.reshape([-1])\n",
    "                stds = np.sqrt(self.models[idx].covariances_).reshape([-1])\n",
    "\n",
    "                result =np.zeros_like(feature_val)\n",
    "\n",
    "                for index in range(len(data)):\n",
    "                    if p_argmax[index] < len(info['modal']):\n",
    "                        argmax_val = p_argmax[index]\n",
    "                        result[index] = float(list(map(info['modal'].__getitem__,[argmax_val]))[0])\n",
    "                    else:\n",
    "                        std = stds[(p_argmax[index] - len(info['modal']))]\n",
    "                        mean = means[(p_argmax[index] - len(info['modal']))]\n",
    "                        result[index] = feature_val[index] * 4 * std + mean\n",
    "                \n",
    "                data_trans_back[:,idx] = result\n",
    "\n",
    "                step += 1 + np.sum(self.components[idx]) + len(info['modal'])\n",
    "\n",
    "            else: # pure category\n",
    "\n",
    "                cate_onehot = data[:,step:step + info['size']]\n",
    "                index = np.argmax(cate_onehot,axis=1)\n",
    "\n",
    "                data_trans_back[:,idx] = list(map(info['i2s'].__getitem__,index))\n",
    "                step += info['size']\n",
    "        \n",
    "        return data_trans_back\n",
    "\n",
    "class Img_Transformer():\n",
    "\n",
    "    def __init__(self,side):\n",
    "        self.height = side\n",
    "    \n",
    "    def transform(self,data):\n",
    "\n",
    "        if self.height * self.height > len(data[0]):\n",
    "            # 表格特征维度不够\n",
    "            padding = torch.zeros((len(data),self.height * self.height - len(data[0]))).to(data.device)\n",
    "            data = torch.cat([data,padding],axis=1)\n",
    "        \n",
    "        return data.view(-1,1,self.height,self.height)\n",
    "    \n",
    "    def inverse_transform(self,data):\n",
    "        data = data.view(-1,self.height * self.height)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import (Dropout, LeakyReLU, Linear, ReLU, Sequential,\n",
    "Conv2d, ConvTranspose2d, BatchNorm2d, Sigmoid, init, BCELoss, CrossEntropyLoss,SmoothL1Loss)\n",
    "\n",
    "def col_based_sampling(probs,col_idx):\n",
    "\n",
    "    option_list =[]\n",
    "\n",
    "    for i in col_idx:\n",
    "        p = probs[i] + 1e-5\n",
    "        p = p/sum(p)\n",
    "\n",
    "        option_list.append(np.random.choice(np.arange(len(probs[i])),p=p))\n",
    "\n",
    "    return np.array(option_list).reshape(col_idx.shape)\n",
    "\n",
    "def cond_loss(data,output_info,cond_vec,m):\n",
    "\n",
    "    tmp_loss = []\n",
    "    \n",
    "    start = 0\n",
    "    start_cond = 0\n",
    "\n",
    "    for item in output_info:\n",
    "        if item[1] == 'tanh':\n",
    "            start += item[0]\n",
    "            continue\n",
    "        elif item[1] == 'softmax':\n",
    "            end = start + item[0]\n",
    "            end_cond = start_cond + item[0]\n",
    "\n",
    "            tmp = F.cross_entropy(data[:,start:end],\n",
    "                                  torch.argmax(cond_vec[:,start_cond:end_cond],dim=1),\n",
    "                                  reduction='none')\n",
    "            tmp_loss.append(tmp)\n",
    "\n",
    "            start = end\n",
    "            start_cond = end_cond\n",
    "    \n",
    "    tmp_loss = torch.stack(tmp_loss,axis=1)\n",
    "    loss = (tmp_loss * m).sum() / data.size()[0]\n",
    "\n",
    "    return loss\n",
    "\n",
    "def get_start_end(target_col_idx,ouptut_info):\n",
    "\n",
    "    start = 0\n",
    "    \n",
    "    cate_c = 0\n",
    "    all_c = 0\n",
    "\n",
    "    for item in ouptut_info:\n",
    "        if cate_c == target_col_idx:\n",
    "            break\n",
    "        if item[1] == 'tanh':\n",
    "            start += item[0]\n",
    "        elif item[1] == 'softmax':\n",
    "            start += item[0]\n",
    "            cate_c += 1\n",
    "        all_c += 1\n",
    "    \n",
    "    end = start + ouptut_info[all_c][0]\n",
    "\n",
    "    return (start,end)\n",
    "\n",
    "def apply_activate(data,output_info):\n",
    "\n",
    "    data_t = list()\n",
    "\n",
    "    start = 0\n",
    "    for item in output_info:\n",
    "        if item[1] == 'tanh':\n",
    "            end = start + item[0]\n",
    "            data_t.append(torch.tanh(data[:,start:end]))\n",
    "            start = end\n",
    "        elif item[1] == 'softmax':\n",
    "            end = start + item[0]\n",
    "            # here use gumbel_softmax since argmax operation is not differentiable\n",
    "            data_t.append(F.gumbel_softmax(data[:,start:end],tau=0.2))\n",
    "            start = end\n",
    "    \n",
    "    act_data = torch.cat(data_t,dim=1)\n",
    "\n",
    "    return act_data\n",
    "\n",
    "def weights_init(model):\n",
    "    classname = model.__class__.__name__\n",
    "\n",
    "    if classname.find('Conv') != -1:\n",
    "        init.normal_(model.weight.data,0.0,0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        init.normal_(model.weight.data,1.0,0.02)\n",
    "        init.constant_(model.bias.data,0)\n",
    "\n",
    "def build_discriminator_layers(side,num_channels):\n",
    "    \n",
    "    layer_dims = [(1,side),(num_channels,side//2)]\n",
    "\n",
    "    while layer_dims[-1][1] > 3 and len(layer_dims) < 4:\n",
    "        layer_dims.append((layer_dims[-1][0] * 2,layer_dims[-1][1] // 2))\n",
    "    \n",
    "    layers_D = list()\n",
    "    for prev,curr in zip(layer_dims,layer_dims[1:]):\n",
    "        layers_D += [Conv2d(prev[0],curr[0],4,2,1,bias=False), # 卷积核=4,步长=2,padd=1 保证图像尺寸 * 1/2\n",
    "                     BatchNorm2d(curr[0]),\n",
    "                     LeakyReLU(0.2,inplace=True)]\n",
    "    \n",
    "    # last layer 输出一个numric value --- use sigmoid()\n",
    "    layers_D += [Conv2d(layer_dims[-1][0],1,layer_dims[-1][1],1,0),Sigmoid()]\n",
    "\n",
    "    return layers_D\n",
    "\n",
    "def build_generator_layers(side,rand_dim,num_channels):\n",
    "\n",
    "    layer_dims = [(1,side),(num_channels,side // 2)]\n",
    "\n",
    "    while layer_dims[-1][1] > 3 and len(layer_dims) < 4:\n",
    "        layer_dims.append((layer_dims[-1][0] * 2,layer_dims[-1][1] // 2))\n",
    "    \n",
    "    layers_G = [ConvTranspose2d(rand_dim,layer_dims[-1][0],layer_dims[-1][1],1,0,output_padding=0,bias=False)]\n",
    "    \n",
    "    for prev,curr in zip(reversed(layer_dims),reversed(layer_dims[:-1])):\n",
    "        layers_G += [BatchNorm2d(prev[0]),ReLU(True),ConvTranspose2d(prev[0],curr[0],4,2,1,output_padding=0,bias=True)]\n",
    "\n",
    "    return layers_G   \n",
    "\n",
    "\n",
    "class Cond_vector():\n",
    "\n",
    "    def __init__(self,data,output_info):\n",
    "        '''\n",
    "        data:transformed data\n",
    "        output_info:\n",
    "        '''\n",
    "\n",
    "        self.model = list()\n",
    "        self.interval = list()\n",
    "        self.n_col = 0 # num of one-hot-encoding representations\n",
    "        self.n_opt = 0 # num of distinct categories across all one-hot-encoding representations\n",
    "        self.p_log_sampling = list()\n",
    "        self.p_sampling = list()\n",
    "\n",
    "        start = 0\n",
    "        for item in output_info:\n",
    "            if item[1] == 'tanh':\n",
    "                start += item[0]\n",
    "                continue\n",
    "            elif item[1] == 'softmax':\n",
    "                end = start + item[0]\n",
    "                self.model.append(np.argmax(data[:,start:end],axis=-1))\n",
    "                self.interval.append((self.n_opt,item[0]))\n",
    "\n",
    "                self.n_col += 1\n",
    "                self.n_opt += item[0]\n",
    "                \n",
    "                freq = np.sum(data[:,start:end],axis=0)\n",
    "                log_freq = np.log(freq + 1)\n",
    "                log_pmf = log_freq / np.sum(log_freq)\n",
    "                self.p_log_sampling.append(log_pmf)\n",
    "                \n",
    "                pmf = freq / np.sum(freq)\n",
    "                self.p_sampling.append(pmf)\n",
    "\n",
    "                start = end\n",
    "        \n",
    "        self.interval = np.asarray(self.interval)\n",
    "\n",
    "    def train_sample(self,batch):\n",
    "\n",
    "        if self.n_col == 0:\n",
    "            return None\n",
    "        batch = batch\n",
    "\n",
    "        cond_vec = np.zeros((batch,self.n_opt),dtype = 'float32')\n",
    "\n",
    "        idx = np.random.choice(np.arange(self.n_col),batch)\n",
    "\n",
    "        mask = np.zeros((batch,self.n_col),dtype='float32')\n",
    "        mask[np.arange(batch),idx] = 1\n",
    "\n",
    "        category_val = col_based_sampling(self.p_log_sampling,idx)\n",
    "\n",
    "        for i in np.arange(batch):\n",
    "            cond_vec[i,self.interval[idx[i],0] + category_val[i]] = 1\n",
    "        \n",
    "        return cond_vec,mask,idx,category_val\n",
    "    \n",
    "    def sample(self,batch):\n",
    "\n",
    "        if self.n_col == 0:\n",
    "            return None\n",
    "        \n",
    "        batch = batch\n",
    "\n",
    "        cond_vec = np.zeros((batch,self.n_opt),dtype='float32')\n",
    "\n",
    "        idx = np.random.choice(np.arange(self.n_col),batch)\n",
    "\n",
    "        category_val = col_based_sampling(self.p_sampling,idx)\n",
    "\n",
    "        for i in np.arange(batch):\n",
    "            cond_vec[i,self.interval[idx[i],0] + category_val[i]] = 1\n",
    "        \n",
    "        return cond_vec\n",
    "\n",
    "class Sampler():\n",
    "    # 抽样特定condition vector的样本\n",
    "    \n",
    "    def __init__(self,data,output_info):\n",
    "        super(Sampler,self).__init__()\n",
    "\n",
    "        self.data = data\n",
    "        self.model = []\n",
    "        self.n =len(data)\n",
    "\n",
    "        start = 0\n",
    "        for item in output_info:\n",
    "            if item[1] == 'tanh':\n",
    "                start += item[0]\n",
    "                continue\n",
    "            elif item[1] == 'softmax':\n",
    "                end = start + item[0]\n",
    "                tmp = []\n",
    "                for j in range(item[0]):\n",
    "                    tmp.append(np.nonzero(data[:,start + j])[0])\n",
    "\n",
    "                self.model.append(tmp)\n",
    "                start = end\n",
    "    \n",
    "    def sample(self,n,col,opt):\n",
    "\n",
    "        if col is None:\n",
    "            idx = np.random.choice(np.arange(self.n),n)\n",
    "            return self.data[idx]\n",
    "        \n",
    "        idx = []\n",
    "        for c,v in zip(col,opt):\n",
    "            idx.append(np.random.choice(self.model[c][v]))\n",
    "        \n",
    "        return self.data[idx]\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self,input_dim,class_dims,start_end):\n",
    "        super(Classifier,self).__init__()\n",
    "        self.input_dim = input_dim - (start_end[1] - start_end[0])\n",
    "        self.class_dims = class_dims # hidden layer dims\n",
    "        self.start_end = start_end\n",
    "\n",
    "        layer_seq = list()\n",
    "        hid_dim = self.input_dim\n",
    "\n",
    "        for i in list(self.class_dims):\n",
    "            layer_seq += [Linear(hid_dim,i),LeakyReLU(0.3),Dropout(0.5)]\n",
    "            hid_dim = i\n",
    "        \n",
    "        if (start_end[1] - start_end[0]) == 2: # 2分类\n",
    "            layer_seq += [Linear(hid_dim,1),Sigmoid()]\n",
    "        else: # 多分类\n",
    "            layer_seq += [Linear(hid_dim,(start_end[1]-start_end[0]))]\n",
    "        \n",
    "        self.seq = Sequential(*layer_seq)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \n",
    "        label = torch.argmax(x[:,self.start_end[0]:self.start_end[1]],axis=1)\n",
    "\n",
    "        x_ = torch.cat((x[:,:self.start_end[0]],x[:,self.start_end[1]:]),1)\n",
    "\n",
    "        if ((self.start_end[1] - self.start_end[0])==2):\n",
    "            return self.seq(x_).view(-1),label\n",
    "        else:\n",
    "            return self.seq(x_),label\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self,layers):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.seq = Sequential(*layers)\n",
    "        self.seq_info = Sequential(*layers[:len(layers)-2])\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return (self.seq(x)),self.seq_info(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self,layers):\n",
    "        super(Generator,self).__init__()\n",
    "        self.seq = Sequential(*layers)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.seq(x)\n",
    "\n",
    "class Faker():\n",
    "\n",
    "    def __init__(self,\n",
    "                 rand_dim = 100,\n",
    "                 class_dim = (256,256,256,256),\n",
    "                 num_channels = 64,\n",
    "                 l2scale = 1e-5,\n",
    "                 batch_size = 512,\n",
    "                 epochs=1):\n",
    "        \n",
    "        self.rand_dim = rand_dim\n",
    "        self.class_dim = class_dim\n",
    "        self.num_channels = num_channels\n",
    "        self.d_side = None\n",
    "        self.g_side = None\n",
    "        self.l2scale = l2scale\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.generator = None\n",
    "        self.discriminator = None\n",
    "        self.classifier = None\n",
    "    \n",
    "    def fit(self,train_data = pd.DataFrame,categorial=[],mixed={},ml_task={}):\n",
    "\n",
    "        problem_type = None\n",
    "        target_index = None\n",
    "\n",
    "        if ml_task:\n",
    "            problem_type = list(ml_task.keys())[0]\n",
    "            if problem_type:\n",
    "                target_index = train_data.columns.get_loc(ml_task[problem_type])\n",
    "        \n",
    "        self.transformer = D_Transformer(train_data=train_data,categorical_cols=categorial,mixed_dict=mixed)\n",
    "        self.transformer.fit()\n",
    "\n",
    "        # 编码转换\n",
    "        train_data = self.transformer.transform(train_data.values)\n",
    "        data_dim = self.transformer.output_dim\n",
    "\n",
    "        # init\n",
    "        data_sampler = Sampler(train_data,self.transformer.output_info)\n",
    "        self.cond_generator = Cond_vector(train_data,self.transformer.output_info)\n",
    "\n",
    "        # for discriminator\n",
    "        sides_d = [4,8,16,24,32,48]\n",
    "        col_size_d = data_dim + self.cond_generator.n_opt\n",
    "        for i in sides_d:\n",
    "            if i**2 >= col_size_d:\n",
    "                self.d_side = i\n",
    "                break\n",
    "        \n",
    "        # for generator\n",
    "        sides_g = [4,8,16,24,32,48]\n",
    "        col_size_g = data_dim\n",
    "        for i in sides_g:\n",
    "            if i**2 >= col_size_g:\n",
    "                self.g_side = i\n",
    "                break\n",
    "        \n",
    "        layers_G = build_generator_layers(self.g_side,self.rand_dim+self.cond_generator.n_opt,self.num_channels)\n",
    "        layers_D = build_discriminator_layers(self.d_side,self.num_channels)\n",
    "\n",
    "        self.generator = Generator(layers_G).to(self.device)\n",
    "        self.discriminator = Discriminator(layers_D).to(self.device)\n",
    "\n",
    "        opt_params = dict(lr=1e-5, betas=(.5,.9), eps=1e-3, weight_decay=self.l2scale)\n",
    "        opt_G = Adam(self.generator.parameters(),**opt_params)\n",
    "        opt_D = Adam(self.discriminator.parameters(),**opt_params)\n",
    "\n",
    "\n",
    "        # target col\n",
    "        start_end = None\n",
    "        classifier = None\n",
    "        opt_C = None\n",
    "\n",
    "        if target_index:\n",
    "            start_end = get_start_end(target_index,self.transformer.output_info)\n",
    "            \n",
    "            self.classifier = Classifier(data_dim,self.class_dim,start_end).to(self.device)\n",
    "            opt_C = Adam(self.classifier.parameters(),**opt_params)\n",
    "        \n",
    "        self.generator.apply(weights_init)\n",
    "        self.discriminator.apply(weights_init)\n",
    "\n",
    "        # init img transformer\n",
    "        self.G_transformer = Img_Transformer(self.g_side)\n",
    "        self.D_transformer = Img_Transformer(self.d_side)\n",
    "\n",
    "\n",
    "        steps_per_epoch = max(1,len(train_data)) // self.batch_size\n",
    "        for i in tqdm(range(self.epochs)):\n",
    "            for _ in range(steps_per_epoch):\n",
    "\n",
    "                noisez = torch.randn(self.batch_size,self.rand_dim,device=self.device)\n",
    "                cond_vec = self.cond_generator.train_sample(self.batch_size)\n",
    "\n",
    "                c,m,col,opt = cond_vec\n",
    "                c = torch.from_numpy(c).to(self.device)\n",
    "                m = torch.from_numpy(m).to(self.device)\n",
    "\n",
    "                noisez = torch.cat([noisez,c], dim=1) # 合并 noise & cond_vec\n",
    "                noisez = noisez.view(self.batch_size,self.rand_dim+self.cond_generator.n_opt,1,1)\n",
    "\n",
    "                real_idx = np.arange(self.batch_size)\n",
    "                np.random.shuffle(real_idx)\n",
    "                real = data_sampler.sample(self.batch_size,col[real_idx],opt[real_idx])\n",
    "                real = torch.from_numpy(real.astype('float32')).to(self.device)\n",
    "\n",
    "                c_real = c[real_idx]\n",
    "\n",
    "                # use generator to generate fake images\n",
    "                fake = self.generator(noisez)\n",
    "                # transform it into table \n",
    "                fake_t = self.G_transformer.inverse_transform(fake)\n",
    "                # apply activation\n",
    "                act_fake_t = apply_activate(fake_t,self.transformer.output_info)\n",
    "                \n",
    "                # concate with cond vec\n",
    "                fake_cat = torch.cat([act_fake_t,c],dim=1)\n",
    "                real_cat = torch.cat([real,c_real],dim=1)\n",
    "\n",
    "                fake_cat_d = self.D_transformer.transform(fake_cat)\n",
    "                real_cat_d = self.D_transformer.transform(real_cat)\n",
    "\n",
    "                ################################# update Discriminator ##################################\n",
    "\n",
    "                # delete cum gradient before each gradient descent\n",
    "                opt_D.zero_grad()\n",
    "                # apply discriminator to real input & fake input\n",
    "                y_real,_ = self.discriminator(real_cat_d)\n",
    "                y_fake,_ = self.discriminator(fake_cat_d)\n",
    "\n",
    "                # compute loss for discriminator of GAN --- max likelihood log(D(x)) + log(1 - D(G(z))) \n",
    "                # ---> min loss_d = -log(D(x)) - log(1-D(G(z)))\n",
    "                loss_d = (-(torch.log(y_real + 1e-4).mean()) - (torch.log(1. - y_fake + 1e-4).mean()))\n",
    "\n",
    "                loss_d.backward() # bp\n",
    "                opt_D.step() # gradient update\n",
    "\n",
    "                ##################################### Done #############################################\n",
    "\n",
    "                noisez = torch.randn(self.batch_size,self.rand_dim,device=self.device)\n",
    "                cond_vec = self.cond_generator.train_sample(self.batch_size)\n",
    "\n",
    "                c,m,col,opt = cond_vec\n",
    "                c = torch.from_numpy(c).to(self.device)\n",
    "                m = torch.from_numpy(m).to(self.device)\n",
    "\n",
    "                noisez = torch.cat([noisez,c], dim=1) # 合并 noise & cond_vec\n",
    "                noisez = noisez.view(self.batch_size,self.rand_dim+self.cond_generator.n_opt,1,1)\n",
    "\n",
    "                ################################### update Generator ###################################\n",
    "\n",
    "                opt_G.zero_grad()\n",
    "\n",
    "                fake = self.generator(noisez)\n",
    "                fake_t = self.G_transformer.inverse_transform(fake)\n",
    "                act_fake_t = apply_activate(fake_t,self.transformer.output_info)\n",
    "                fake_cat = torch.cat([act_fake_t,c],dim=1)\n",
    "                fake_cat_d = self.D_transformer.transform(fake_cat)\n",
    "\n",
    "                y_fake,info_fake = self.discriminator(fake_cat_d)\n",
    "                _,info_real = self.discriminator(real_cat_d)\n",
    "                conditional_loss = cond_loss(fake_t,self.transformer.output_info,c,m)\n",
    "\n",
    "                # generator loss part1 = condition loss + - log(D(G(z)))\n",
    "                loss_g = conditional_loss - (torch.log(y_fake + 1e-5).mean())\n",
    "                loss_g.backward(retain_graph=True) # keep computation graph since we also have the information loss for gradient descent\n",
    "\n",
    "                # generator loss part2 = information loss \n",
    "                # here we check the first-order and second-order statistics of fake data & real data\n",
    "                loss_mean = torch.norm(torch.mean(info_fake.view(self.batch_size,-1),dim=0) - torch.mean(info_real.view(self.batch_size,-1),dim=0),1)\n",
    "                loss_std = torch.norm(torch.std(info_fake.view(self.batch_size,-1),dim=0) - torch.std(info_real.view(self.batch_size,-1),dim=0),1)\n",
    "\n",
    "                loss_info = loss_mean + loss_std\n",
    "\n",
    "                loss_info.backward()\n",
    "\n",
    "                opt_G.step()\n",
    "\n",
    "\n",
    "                # check the target column for classification loss\n",
    "                if problem_type:\n",
    "\n",
    "                    c_loss = None\n",
    "\n",
    "                    if (start_end[1] - start_end[0]) == 2: # binary classification problem\n",
    "                        c_loss = BCELoss()\n",
    "                    else: # multi-class \n",
    "                        c_loss = CrossEntropyLoss() \n",
    "                    \n",
    "                    opt_C.zero_grad()\n",
    "                    real_pred,real_label = self.classifier(real)\n",
    "                    if (start_end[1] - start_end[0]) == 2:\n",
    "                        real_label = real_label.type_as(real_pred)\n",
    "                    \n",
    "                    loss_classification = c_loss(real_pred,real_label)\n",
    "                    loss_classification.backward()\n",
    "                    opt_C.step()\n",
    "\n",
    "\n",
    "                    # also update the weight of generator\n",
    "                    opt_G.zero_grad()\n",
    "                    fake = self.generator(noisez)\n",
    "                    fake_t = self.G_transformer.inverse_transform(fake)\n",
    "                    act_fake_t = apply_activate(fake_t,self.transformer.output_info)\n",
    "\n",
    "                    fake_pred,fake_label = self.classifier(act_fake_t)\n",
    "                    if (start_end[1] - start_end[0]) == 2:\n",
    "                        fake_label = fake_label.type_as(fake_pred)\n",
    "                    \n",
    "                    loss_classification_g = c_loss(fake_pred,fake_label)\n",
    "                    loss_classification_g.backward()\n",
    "                    opt_G.step()\n",
    "\n",
    "    def sample(self,num):\n",
    "        \n",
    "        # turn the generator into inference mode\n",
    "        self.generator.eval()\n",
    "        # col info\n",
    "        output_info = self.transformer.output_info\n",
    "\n",
    "        steps = num // self.batch_size + 1\n",
    "        data = []\n",
    "\n",
    "        for _ in range(steps):\n",
    "            noisez = torch.randn(self.batch_size,self.rand_dim,device=self.device)\n",
    "            cond_vec = self.cond_generator.sample(self.batch_size)\n",
    "            c = cond_vec\n",
    "            c = torch.from_numpy(c).to(self.device)\n",
    "            noisez = torch.cat([noisez,c],dim=1)\n",
    "            noisez = noisez.view(self.batch_size,self.rand_dim+self.cond_generator.n_opt,1,1)\n",
    "            \n",
    "            fake = self.generator(noisez)\n",
    "            fake_t = self.G_transformer.inverse_transform(fake)\n",
    "            act_fake_t = apply_activate(fake_t,output_info)\n",
    "\n",
    "            data.append(act_fake_t.detach().cpu().numpy())\n",
    "        \n",
    "        data = np.concatenate(data,axis=0)\n",
    "        result = self.transformer.inverse_transform(data)\n",
    "\n",
    "        return result[0:num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class Let_us_fake():\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_path,\n",
    "                 test_ratio,\n",
    "                 categorical_columns,\n",
    "                 log_columns,\n",
    "                 mixed_columns,\n",
    "                 integer_columns,\n",
    "                 problem_type,\n",
    "                 epochs):\n",
    "        \n",
    "        self.faker = Faker(epochs=epochs)\n",
    "        self.raw_df = pd.read_csv(data_path)\n",
    "        self.test_ratio = test_ratio\n",
    "        self.categorical_columns = categorical_columns\n",
    "        self.log_columns = log_columns\n",
    "        self.mixed_columns = mixed_columns\n",
    "        self.integer_columns = integer_columns\n",
    "        self.problem_type = problem_type\n",
    "\n",
    "        self.epochs = epochs\n",
    "    \n",
    "    def fit(self):\n",
    "\n",
    "        start_time = time.time()\n",
    "        self.data_prep = Data_Prep(self.raw_df,self.categorical_columns,self.log_columns,self.mixed_columns,self.integer_columns,self.problem_type,self.test_ratio)\n",
    "        print('data preparation done.')\n",
    "\n",
    "\n",
    "        print('start training...')\n",
    "        self.faker.fit(train_data=self.data_prep.df,\n",
    "                       categorial=self.data_prep.col_types['categorical'],\n",
    "                       mixed = self.data_prep.col_types['mixed'],\n",
    "                       ml_task=self.problem_type)\n",
    "        \n",
    "        print('finish training in {} seconds.'.format(time.time() - start_time))\n",
    "\n",
    "\n",
    "    def generate_samples(self):\n",
    "\n",
    "        sample = self.faker.sample(len(self.raw_df))\n",
    "        sample_df = self.data_prep.inverse_prep(sample)\n",
    "\n",
    "        return sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始构造虚假人造样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_y_tmp = train_x_y.copy()\n",
    "train_x_y_tmp.drop(columns=['month','cust_no','mob3_k4','fpd4'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_columns = {}\n",
    "for f in list(train_x_y_tmp)[:-1]:\n",
    "    if -99 in train_x_y_tmp[f].values:\n",
    "        mixed_columns[f] = [-99]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_y_tmp.to_csv('real_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "faker_object = Let_us_fake(data_path='real_data.csv',\n",
    "                           test_ratio=0.2,\n",
    "                           categorical_columns=[],\n",
    "                           log_columns=[],\n",
    "                           mixed_columns=mixed_columns,\n",
    "                           integer_columns=[],\n",
    "                           problem_type={'classification':'mob3_k11'},\n",
    "                           epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data preparation done.\n",
      "start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [17:19<00:00, 1039.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish training in 1444.1361246109009 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "faker_object.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = faker_object.generate_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pic_zm_score</th>\n",
       "      <th>pic_zm_upload_source</th>\n",
       "      <th>bqs_qly_feature18</th>\n",
       "      <th>bqs_qly_feature35</th>\n",
       "      <th>bqs_qly_feature13</th>\n",
       "      <th>idcard_district</th>\n",
       "      <th>upa_max_score</th>\n",
       "      <th>upa_total_deal_amt_low_limit_12m</th>\n",
       "      <th>upa_recent_deal_date_12m</th>\n",
       "      <th>upa_earliest_deal_date_from_201101</th>\n",
       "      <th>...</th>\n",
       "      <th>new_reflecting_back_id_score</th>\n",
       "      <th>new_reflecting_front_id_score</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>area_risk_level</th>\n",
       "      <th>cust_birth_date</th>\n",
       "      <th>selffill_marital_status</th>\n",
       "      <th>app_sum_cnt</th>\n",
       "      <th>cust_id_area</th>\n",
       "      <th>cust_work_city</th>\n",
       "      <th>mob3_k11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-99.000000</td>\n",
       "      <td>0.116741</td>\n",
       "      <td>492.060181</td>\n",
       "      <td>20.624346</td>\n",
       "      <td>56.945354</td>\n",
       "      <td>423682.781250</td>\n",
       "      <td>10.830812</td>\n",
       "      <td>97.179955</td>\n",
       "      <td>20210520.0</td>\n",
       "      <td>20116696.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.909511</td>\n",
       "      <td>0.054651</td>\n",
       "      <td>1991.418457</td>\n",
       "      <td>4.033495</td>\n",
       "      <td>19890100.0</td>\n",
       "      <td>10.072654</td>\n",
       "      <td>68.922943</td>\n",
       "      <td>420594.96875</td>\n",
       "      <td>630362.000000</td>\n",
       "      <td>-0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-99.000000</td>\n",
       "      <td>1.003084</td>\n",
       "      <td>12.942337</td>\n",
       "      <td>35.419430</td>\n",
       "      <td>21.137197</td>\n",
       "      <td>225607.859375</td>\n",
       "      <td>744.860046</td>\n",
       "      <td>-43.166283</td>\n",
       "      <td>20210610.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>1992.995239</td>\n",
       "      <td>1.122906</td>\n",
       "      <td>19841468.0</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>197.580948</td>\n",
       "      <td>503981.96875</td>\n",
       "      <td>514139.343750</td>\n",
       "      <td>1.047541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-99.000000</td>\n",
       "      <td>1.003688</td>\n",
       "      <td>492.084656</td>\n",
       "      <td>20.609312</td>\n",
       "      <td>187.332489</td>\n",
       "      <td>339011.375000</td>\n",
       "      <td>970.134216</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>20200532.0</td>\n",
       "      <td>20159992.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020258</td>\n",
       "      <td>0.002849</td>\n",
       "      <td>1989.579102</td>\n",
       "      <td>1.104489</td>\n",
       "      <td>19996926.0</td>\n",
       "      <td>10.067298</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>169669.59375</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-0.000814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>679.147705</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>214.419998</td>\n",
       "      <td>-0.220303</td>\n",
       "      <td>167.772278</td>\n",
       "      <td>369967.375000</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-329.840820</td>\n",
       "      <td>20210306.0</td>\n",
       "      <td>20148650.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047013</td>\n",
       "      <td>0.958308</td>\n",
       "      <td>2000.596069</td>\n",
       "      <td>2.016736</td>\n",
       "      <td>19970578.0</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>41.304680</td>\n",
       "      <td>520842.84375</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-0.000270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.782473</td>\n",
       "      <td>1.005619</td>\n",
       "      <td>6.420647</td>\n",
       "      <td>20.139893</td>\n",
       "      <td>166.025787</td>\n",
       "      <td>333867.718750</td>\n",
       "      <td>13.291022</td>\n",
       "      <td>1394.853027</td>\n",
       "      <td>20210680.0</td>\n",
       "      <td>20160078.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066810</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>1984.004150</td>\n",
       "      <td>3.021808</td>\n",
       "      <td>19810784.0</td>\n",
       "      <td>10.050948</td>\n",
       "      <td>188.149200</td>\n",
       "      <td>529405.25000</td>\n",
       "      <td>629792.875000</td>\n",
       "      <td>-0.000206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25067</th>\n",
       "      <td>4.091121</td>\n",
       "      <td>2.010348</td>\n",
       "      <td>44.887699</td>\n",
       "      <td>126.567802</td>\n",
       "      <td>169.732437</td>\n",
       "      <td>368458.062500</td>\n",
       "      <td>7.651791</td>\n",
       "      <td>8175.944336</td>\n",
       "      <td>20220066.0</td>\n",
       "      <td>20170432.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002159</td>\n",
       "      <td>0.764762</td>\n",
       "      <td>1996.953613</td>\n",
       "      <td>4.002738</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>20.069139</td>\n",
       "      <td>90.714020</td>\n",
       "      <td>518918.12500</td>\n",
       "      <td>619893.500000</td>\n",
       "      <td>-0.000074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25068</th>\n",
       "      <td>626.089966</td>\n",
       "      <td>0.080011</td>\n",
       "      <td>81.886360</td>\n",
       "      <td>0.052379</td>\n",
       "      <td>158.979065</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>14.447273</td>\n",
       "      <td>31974.728516</td>\n",
       "      <td>20220056.0</td>\n",
       "      <td>20170388.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019285</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>1997.888062</td>\n",
       "      <td>4.020222</td>\n",
       "      <td>19823118.0</td>\n",
       "      <td>10.032281</td>\n",
       "      <td>85.388985</td>\n",
       "      <td>433786.25000</td>\n",
       "      <td>137400.203125</td>\n",
       "      <td>-0.002603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25069</th>\n",
       "      <td>-99.000000</td>\n",
       "      <td>0.073892</td>\n",
       "      <td>213.453568</td>\n",
       "      <td>12.783333</td>\n",
       "      <td>64.104752</td>\n",
       "      <td>348966.875000</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>92.342216</td>\n",
       "      <td>20210644.0</td>\n",
       "      <td>20146400.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>1986.561279</td>\n",
       "      <td>5.165443</td>\n",
       "      <td>19996948.0</td>\n",
       "      <td>20.081871</td>\n",
       "      <td>82.774811</td>\n",
       "      <td>452779.78125</td>\n",
       "      <td>161906.843750</td>\n",
       "      <td>0.985558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25070</th>\n",
       "      <td>-99.000000</td>\n",
       "      <td>2.039971</td>\n",
       "      <td>45.402184</td>\n",
       "      <td>12.713758</td>\n",
       "      <td>0.768507</td>\n",
       "      <td>339764.875000</td>\n",
       "      <td>796.271057</td>\n",
       "      <td>1315.403198</td>\n",
       "      <td>20200946.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013953</td>\n",
       "      <td>0.906269</td>\n",
       "      <td>1998.246582</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>19912826.0</td>\n",
       "      <td>10.046247</td>\n",
       "      <td>61.303860</td>\n",
       "      <td>349691.21875</td>\n",
       "      <td>617033.125000</td>\n",
       "      <td>0.000502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25071</th>\n",
       "      <td>673.212402</td>\n",
       "      <td>2.010869</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>122.698509</td>\n",
       "      <td>100.297234</td>\n",
       "      <td>220186.656250</td>\n",
       "      <td>747.742493</td>\n",
       "      <td>-179.363678</td>\n",
       "      <td>20210090.0</td>\n",
       "      <td>20187058.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262481</td>\n",
       "      <td>0.055041</td>\n",
       "      <td>1989.500610</td>\n",
       "      <td>1.055883</td>\n",
       "      <td>19836226.0</td>\n",
       "      <td>10.034551</td>\n",
       "      <td>76.991859</td>\n",
       "      <td>500427.90625</td>\n",
       "      <td>617369.250000</td>\n",
       "      <td>0.000340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25072 rows × 111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pic_zm_score  pic_zm_upload_source  bqs_qly_feature18  \\\n",
       "0        -99.000000              0.116741         492.060181   \n",
       "1        -99.000000              1.003084          12.942337   \n",
       "2        -99.000000              1.003688         492.084656   \n",
       "3        679.147705            -99.000000         214.419998   \n",
       "4          6.782473              1.005619           6.420647   \n",
       "...             ...                   ...                ...   \n",
       "25067      4.091121              2.010348          44.887699   \n",
       "25068    626.089966              0.080011          81.886360   \n",
       "25069    -99.000000              0.073892         213.453568   \n",
       "25070    -99.000000              2.039971          45.402184   \n",
       "25071    673.212402              2.010869         -99.000000   \n",
       "\n",
       "       bqs_qly_feature35  bqs_qly_feature13  idcard_district  upa_max_score  \\\n",
       "0              20.624346          56.945354    423682.781250      10.830812   \n",
       "1              35.419430          21.137197    225607.859375     744.860046   \n",
       "2              20.609312         187.332489    339011.375000     970.134216   \n",
       "3              -0.220303         167.772278    369967.375000     -99.000000   \n",
       "4              20.139893         166.025787    333867.718750      13.291022   \n",
       "...                  ...                ...              ...            ...   \n",
       "25067         126.567802         169.732437    368458.062500       7.651791   \n",
       "25068           0.052379         158.979065       -99.000000      14.447273   \n",
       "25069          12.783333          64.104752    348966.875000     -99.000000   \n",
       "25070          12.713758           0.768507    339764.875000     796.271057   \n",
       "25071         122.698509         100.297234    220186.656250     747.742493   \n",
       "\n",
       "       upa_total_deal_amt_low_limit_12m  upa_recent_deal_date_12m  \\\n",
       "0                             97.179955                20210520.0   \n",
       "1                            -43.166283                20210610.0   \n",
       "2                            -99.000000                20200532.0   \n",
       "3                           -329.840820                20210306.0   \n",
       "4                           1394.853027                20210680.0   \n",
       "...                                 ...                       ...   \n",
       "25067                       8175.944336                20220066.0   \n",
       "25068                      31974.728516                20220056.0   \n",
       "25069                         92.342216                20210644.0   \n",
       "25070                       1315.403198                20200946.0   \n",
       "25071                       -179.363678                20210090.0   \n",
       "\n",
       "       upa_earliest_deal_date_from_201101  ...  new_reflecting_back_id_score  \\\n",
       "0                              20116696.0  ...                      0.909511   \n",
       "1                                   -99.0  ...                    -99.000000   \n",
       "2                              20159992.0  ...                      0.020258   \n",
       "3                              20148650.0  ...                      0.047013   \n",
       "4                              20160078.0  ...                      0.066810   \n",
       "...                                   ...  ...                           ...   \n",
       "25067                          20170432.0  ...                      0.002159   \n",
       "25068                          20170388.0  ...                      0.019285   \n",
       "25069                          20146400.0  ...                    -99.000000   \n",
       "25070                               -99.0  ...                      0.013953   \n",
       "25071                          20187058.0  ...                      0.262481   \n",
       "\n",
       "       new_reflecting_front_id_score   birth_year  area_risk_level  \\\n",
       "0                           0.054651  1991.418457         4.033495   \n",
       "1                         -99.000000  1992.995239         1.122906   \n",
       "2                           0.002849  1989.579102         1.104489   \n",
       "3                           0.958308  2000.596069         2.016736   \n",
       "4                         -99.000000  1984.004150         3.021808   \n",
       "...                              ...          ...              ...   \n",
       "25067                       0.764762  1996.953613         4.002738   \n",
       "25068                     -99.000000  1997.888062         4.020222   \n",
       "25069                     -99.000000  1986.561279         5.165443   \n",
       "25070                       0.906269  1998.246582       -99.000000   \n",
       "25071                       0.055041  1989.500610         1.055883   \n",
       "\n",
       "       cust_birth_date  selffill_marital_status  app_sum_cnt  cust_id_area  \\\n",
       "0           19890100.0                10.072654    68.922943  420594.96875   \n",
       "1           19841468.0               -99.000000   197.580948  503981.96875   \n",
       "2           19996926.0                10.067298   -99.000000  169669.59375   \n",
       "3           19970578.0               -99.000000    41.304680  520842.84375   \n",
       "4           19810784.0                10.050948   188.149200  529405.25000   \n",
       "...                ...                      ...          ...           ...   \n",
       "25067            -99.0                20.069139    90.714020  518918.12500   \n",
       "25068       19823118.0                10.032281    85.388985  433786.25000   \n",
       "25069       19996948.0                20.081871    82.774811  452779.78125   \n",
       "25070       19912826.0                10.046247    61.303860  349691.21875   \n",
       "25071       19836226.0                10.034551    76.991859  500427.90625   \n",
       "\n",
       "       cust_work_city  mob3_k11  \n",
       "0       630362.000000 -0.000066  \n",
       "1       514139.343750  1.047541  \n",
       "2          -99.000000 -0.000814  \n",
       "3          -99.000000 -0.000270  \n",
       "4       629792.875000 -0.000206  \n",
       "...               ...       ...  \n",
       "25067   619893.500000 -0.000074  \n",
       "25068   137400.203125 -0.002603  \n",
       "25069   161906.843750  0.985558  \n",
       "25070   617033.125000  0.000502  \n",
       "25071   617369.250000  0.000340  \n",
       "\n",
       "[25072 rows x 111 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pic_zm_score</th>\n",
       "      <th>pic_zm_upload_source</th>\n",
       "      <th>bqs_qly_feature18</th>\n",
       "      <th>bqs_qly_feature35</th>\n",
       "      <th>bqs_qly_feature13</th>\n",
       "      <th>idcard_district</th>\n",
       "      <th>upa_max_score</th>\n",
       "      <th>upa_total_deal_amt_low_limit_12m</th>\n",
       "      <th>upa_recent_deal_date_12m</th>\n",
       "      <th>upa_earliest_deal_date_from_201101</th>\n",
       "      <th>...</th>\n",
       "      <th>new_reflecting_back_id_score</th>\n",
       "      <th>new_reflecting_front_id_score</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>area_risk_level</th>\n",
       "      <th>cust_birth_date</th>\n",
       "      <th>selffill_marital_status</th>\n",
       "      <th>app_sum_cnt</th>\n",
       "      <th>cust_id_area</th>\n",
       "      <th>cust_work_city</th>\n",
       "      <th>mob3_k11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>671.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>471.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>440233.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20200229.0</td>\n",
       "      <td>20161205.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.261844e-06</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19970310.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>440233.0</td>\n",
       "      <td>440111.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>411322.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>20201222.0</td>\n",
       "      <td>20201210.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.344012e-06</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19910512.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>411322.0</td>\n",
       "      <td>411322.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>410221.0</td>\n",
       "      <td>589.0</td>\n",
       "      <td>5621.0</td>\n",
       "      <td>20210518.0</td>\n",
       "      <td>20190617.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.856895e-04</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19880105.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>410221.0</td>\n",
       "      <td>110100.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>648.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>492.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>340621.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.187862e-06</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19890910.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>340621.0</td>\n",
       "      <td>340121.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>412827.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.221947e-09</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20000802.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>412827.0</td>\n",
       "      <td>320206.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25067</th>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>370725.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.820533e-05</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19960702.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>370725.0</td>\n",
       "      <td>370702.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25068</th>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>471.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>430703.0</td>\n",
       "      <td>705.0</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>20211130.0</td>\n",
       "      <td>20180505.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.093910e-04</td>\n",
       "      <td>0.041124</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19980121.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>430703.0</td>\n",
       "      <td>430921.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25069</th>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>606.0</td>\n",
       "      <td>20200630.0</td>\n",
       "      <td>20160318.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.611336e-04</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19911113.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>500381.0</td>\n",
       "      <td>500100.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25070</th>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>350781.0</td>\n",
       "      <td>705.0</td>\n",
       "      <td>4631.0</td>\n",
       "      <td>20210410.0</td>\n",
       "      <td>20160525.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.340630e-09</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19911031.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>350781.0</td>\n",
       "      <td>350781.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25071</th>\n",
       "      <td>640.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>996.0</td>\n",
       "      <td>10708.0</td>\n",
       "      <td>20210225.0</td>\n",
       "      <td>20160511.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.740005e-08</td>\n",
       "      <td>0.001464</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19891025.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>460004.0</td>\n",
       "      <td>460107.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25072 rows × 111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pic_zm_score  pic_zm_upload_source  bqs_qly_feature18  \\\n",
       "0             671.0                   1.0              471.0   \n",
       "1             -99.0                 -99.0               19.0   \n",
       "2             -99.0                 -99.0               49.0   \n",
       "3             648.0                   1.0              492.0   \n",
       "4             -99.0                 -99.0               80.0   \n",
       "...             ...                   ...                ...   \n",
       "25067         -99.0                 -99.0              -99.0   \n",
       "25068         -99.0                 -99.0              471.0   \n",
       "25069         -99.0                 -99.0              -99.0   \n",
       "25070         -99.0                 -99.0               79.0   \n",
       "25071         640.0                   1.0              -99.0   \n",
       "\n",
       "       bqs_qly_feature35  bqs_qly_feature13  idcard_district  upa_max_score  \\\n",
       "0                   19.0               18.0         440233.0            9.0   \n",
       "1                   12.0                6.0         411322.0            4.0   \n",
       "2                   23.0               84.0         410221.0          589.0   \n",
       "3                   19.0               36.0         340621.0          -99.0   \n",
       "4                   37.0               53.0         412827.0          -99.0   \n",
       "...                  ...                ...              ...            ...   \n",
       "25067              -99.0              -99.0         370725.0          -99.0   \n",
       "25068               21.0               17.0         430703.0          705.0   \n",
       "25069              -99.0              -99.0            -99.0            4.0   \n",
       "25070               24.0               85.0         350781.0          705.0   \n",
       "25071              -99.0              -99.0            -99.0          996.0   \n",
       "\n",
       "       upa_total_deal_amt_low_limit_12m  upa_recent_deal_date_12m  \\\n",
       "0                                   0.0                20200229.0   \n",
       "1                                1100.0                20201222.0   \n",
       "2                                5621.0                20210518.0   \n",
       "3                                 -99.0                     -99.0   \n",
       "4                                 -99.0                     -99.0   \n",
       "...                                 ...                       ...   \n",
       "25067                             -99.0                     -99.0   \n",
       "25068                            3500.0                20211130.0   \n",
       "25069                             606.0                20200630.0   \n",
       "25070                            4631.0                20210410.0   \n",
       "25071                           10708.0                20210225.0   \n",
       "\n",
       "       upa_earliest_deal_date_from_201101  ...  new_reflecting_back_id_score  \\\n",
       "0                              20161205.0  ...                  6.261844e-06   \n",
       "1                              20201210.0  ...                  2.344012e-06   \n",
       "2                              20190617.0  ...                  7.856895e-04   \n",
       "3                                   -99.0  ...                  9.187862e-06   \n",
       "4                                   -99.0  ...                  3.221947e-09   \n",
       "...                                   ...  ...                           ...   \n",
       "25067                               -99.0  ...                  4.820533e-05   \n",
       "25068                          20180505.0  ...                  8.093910e-04   \n",
       "25069                          20160318.0  ...                  3.611336e-04   \n",
       "25070                          20160525.0  ...                  8.340630e-09   \n",
       "25071                          20160511.0  ...                  1.740005e-08   \n",
       "\n",
       "       new_reflecting_front_id_score  birth_year  area_risk_level  \\\n",
       "0                           0.000224      1997.0              2.0   \n",
       "1                           0.000812      1991.0              5.0   \n",
       "2                           0.004000      1988.0              5.0   \n",
       "3                           0.000819      1989.0              5.0   \n",
       "4                           0.001071      2000.0              5.0   \n",
       "...                              ...         ...              ...   \n",
       "25067                       0.000080      1996.0              5.0   \n",
       "25068                       0.041124      1998.0              3.0   \n",
       "25069                       0.000040      1991.0              4.0   \n",
       "25070                       0.000002      1991.0              2.0   \n",
       "25071                       0.001464      1989.0              2.0   \n",
       "\n",
       "       cust_birth_date  selffill_marital_status  app_sum_cnt  cust_id_area  \\\n",
       "0           19970310.0                     10.0          0.0      440233.0   \n",
       "1           19910512.0                     10.0         47.0      411322.0   \n",
       "2           19880105.0                     20.0         37.0      410221.0   \n",
       "3           19890910.0                     10.0         46.0      340621.0   \n",
       "4           20000802.0                     10.0         39.0      412827.0   \n",
       "...                ...                      ...          ...           ...   \n",
       "25067       19960702.0                     10.0         14.0      370725.0   \n",
       "25068       19980121.0                     20.0          0.0      430703.0   \n",
       "25069       19911113.0                     10.0         38.0      500381.0   \n",
       "25070       19911031.0                     20.0          0.0      350781.0   \n",
       "25071       19891025.0                     20.0          0.0      460004.0   \n",
       "\n",
       "       cust_work_city  mob3_k11  \n",
       "0            440111.0         0  \n",
       "1            411322.0         0  \n",
       "2            110100.0         0  \n",
       "3            340121.0         0  \n",
       "4            320206.0         0  \n",
       "...               ...       ...  \n",
       "25067        370702.0         0  \n",
       "25068        430921.0         0  \n",
       "25069        500100.0         0  \n",
       "25070        350781.0         0  \n",
       "25071        460107.0         0  \n",
       "\n",
       "[25072 rows x 111 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_y_tmp"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
